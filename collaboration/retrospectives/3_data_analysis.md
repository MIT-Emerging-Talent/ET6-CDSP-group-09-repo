# 📊 Retrospective — Milestone 3: Data Analysis

## ✅ What Went Well

---

## ⚠️ What Was Challenging

---

## 💡 What We Learned

---

## 🔄 What We’ll Improve Next

---

## 🎯 Strategy vs. Board Summary

### ✅ As Planned

### ⚠️ What Didn’t Work

### ➕ Additions to Plan

### ➖ Removed or Changed Steps

---

## 👥 Individual Reflections

### 👤 Said Abualroos  

- 🧠 **What I learned**  
  - Model Building: The Universal AI modules and recitations were invaluable
  for understanding how to properly build, test, and interpret models. I learned
  that a high R-squared score can be a red flag for overfitting, and a
  more generalizable model with a lower score is often superior.  
  - Insightful EDA: Stratifying exploratory data analysis by development level
  (SDI) from the beginning is critical. Global averages can mask the most
  important underlying trends in diverse datasets.  
  - Explanatory Power: Replacing hundreds of country-specific dummy variables
  with a single, powerful explanatory variable like SDI leads to a more robust
  and scientifically valid model.  

- ✅ **What went well**  
  - Literature Review: The foundational knowledge gained from the WHO, IDMI, and
  A State of Global Air reports was instrumental in guiding the feature
  engineering and analysis phases.  
  - Modular Workflow: Breaking the analysis into five focused notebooks created
  a clean, reproducible pipeline that was easy to debug and manage.  
  - Iterative Refinement: The step-by-step process of developing the analysis
  allowed for immediate feedback and course correction, which significantly
  improved the final outcome.  

- ⚠️ **What could be improved**  
  - Initial Visualizations: My first attempts at creating time-series plots were
  not insightful. I need to start with stratified or faceted plots in the future
  to get to the core story faster.  
  - Local Automation: I could improve my workflow by using local pre-commit hooks
  to automatically format code. This would prevent CI/CD pipeline
  failures and streamline the development process.  
  - Advanced Modeling: While Random Forest provided a strong baseline, I could
  explore more advanced techniques like Mixed-Effects Models in the future to
  better account for the data's hierarchical structure.  

---

### 👤 Mohamed Tilal  

- 🧠 **What I Learned**  
  - I learned new ML models and applied some modeling techniques like Linear
  Regression and Random Forest, and I practiced doing exploratory data analysis
  (EDA) independently. This helped me better understand the connection
  between data and insights.  

- ✅ **What Went Well**  
  - I contributed to the data analysis, helped in reviewing the old dataset and
  collecting new ones, and reviewed reports to support our modeling decisions.
  The team was mostly active and supportive, which made collaboration
  smooth and productive.  

- ⚠️ **What Could Be Improved**  
  - Applying the models and understanding their outputs was a bit challenging
  at first, but it became easier with practice. Also, a few team members were
  inactive, which slightly affected the overall team flow. I know some of them
  have difficult circumstances and I am totally okay with it, just let the team
  know and stay following the process.  

---

### 👤 Saliha  

- 🧠 **What I Learned**  
  - Thanks to the Universal AI modules and recitations, I was able to better
  understand and apply machine learning analyses. This process was highly
  valuable for me.  
  - Through the feedback provided by my teammates, I learned how to
  improve analyses.  

- ✅ **What Went Well**  
  - After team decisions were made, communication became more fluent, which
  helped tasks proceed quickly and smoothly.  
  - Working iteratively and receiving feedback at each step improved the quality
  of the analysis.  
  - The strategic decisions we made regarding the data and model strengthened
  the scientific soundness of the project.  

- ⚠️ **What Could Be Improved**  
  - I should deepen my knowledge and practice of advanced modeling techniques.  
  - Gaining more experience in model optimization and parameter tuning would
  be beneficial.  
  - Clarifying roles and task distributions earlier would help improve
  workload management and increase overall efficiency.  

---

### 👤 Salih Adam  

- 🧠 **What you learned**  
  - Understanding the dataset and its measurements is critical: Deeply exploring
  what each metric means, how it is measured, and how it relates to real-world
  phenomena is essential for drawing valid conclusions in data analysis.  
  - Practiced computing and interpreting statistical relationships: Learned how
  to calculate and interpret correlation coefficients and p-values, and
  understood their role in identifying potential associations between variables.
  - Recognized the importance of temporal dynamics in data analysis: Became
  aware that outcomes in many datasets may not occur simultaneously with their
  causes or exposures, and that considering time lags is crucial when analyzing
  trends over time.  

- ✅ **What went well**  
  - Clear trend visualizations and correlations computed: The analysis revealed
  meaningful insights about how PM₂.₅ levels relate to health burdens,
  particularly for cardiovascular and respiratory outcomes.  
  - Reproducible pipeline built: The class-based approach allowed for systematic
  data handling, making extraction and analysis flexible and scalable.  

- ⚠️ **What could be improved**  
  - Gain deeper insight into data analysis by completing the Universal AI
  module: Advancing knowledge in machine learning, modeling, and advanced
  analytics will help extend this work beyond basic correlations.  
  - Improve team coordination and communication: Streamlining how the team
  shares progress, discusses findings, and aligns on next steps can enhance
  efficiency and collaboration.  

---

### 👤 Obay Salih  

- 🧠 **What You Learned**  
  - I learned that understanding the type of dataset and its structure is a key
  starting point in any data analysis process. It allows you to identify patterns
  and relationships between variables more effectively.  
  - I also recognized how statistical parameters such as means, correlations,
  and variability are essential tools in exploring and interpreting data,
  especially when working with environmental and health-related indicators.  

- ✅ **What Went Well**  
  - One of the successes during this milestone was being able to translate
  results into clear visualizations. This helped in making
  connections between different variables more intuitive and accessible.  
  - It was rewarding to see how data trends can be communicated visually to
  tell a compelling story.  

- ⚠️ **What Could Be Improved**  
  - I would like to improve my ability to work with different types of data,
  including structured and unstructured datasets.  
  - Understanding how to clean, manipulate, and analyze various data
  formats is something I plan to continue learning.  
  - I also aim to dive deeper into the Universal AI (UAI) course to gain more
  knowledge about advanced data analysis techniques and build more
  confidence in handling complex datasets.  

## 🚀 Next Steps

- 📘 Complete remaining UAI lessons to deepen modeling expertise.
- 📊 Experiment with advanced model types (e.g., mixed-effects, time-series models).
- 🤝 Maintain clear team channels and timelines in future collaborations.
- 🛠️ Set up local development tools (e.g., auto-formatters, linters)
for smoother pipelines.
